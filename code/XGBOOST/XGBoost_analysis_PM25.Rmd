---
title: "AERONET_PMspec_Analysis"
author: "Meytar Sorek-Hamer"
date: "April 11, 2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Libraries
```{r,echo=FALSE,include=FALSE}
library(purrr)
library(tidyr)
library(ggplot2)
library(data.table)
library(dplyr)
library(Hmisc)
library(rlist)
library(xgboost)
library(caret)
library(leaflet)
library(SHAPforxgboost)
```
## Load the working data base
```{r}
#AERONET AOD + EPA CSN PM
#wdb.f<-fread("C:/Users/msorekha/Documents/Projects/JPL/MISR/Data/WDB/AODonly.pm.spec.csv")
#vars <- fread(("C:/Users/msorekha/Documents/Projects/JPL/MISR/Data/WDB/vars.type.spec.csv"))
# aeronet inversion with PM2.5 mass
wdb.f <-fread("/Volumes/GoogleDrive/My Drive/AERONET-MISR/Paper#2-RSoE/aeronet/aeronet/data/inv.pm.data.csv")
#write.csv(names(wdb.f),"/Volumes/GoogleDrive/My Drive/AERONET-MISR/Paper#2-RSoE/aeronet/aeronet/data/inv.vars2.csv",row.names = F)
# replace -999 with NA
wdb.f <- na_if(wdb.f, "-999")
dim(wdb.f)
mean(wdb.f$PM25)
sd(wdb.f$PM25)

# variables for inv xgboost
vars <- fread("/Volumes/GoogleDrive/My Drive/AERONET-MISR/Paper#2-RSoE/aeronet/aeronet/data/inv.vars.pm.csv")

```


# List Directories
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#save results
#res_dir <- "C:/Users/msorekha/Documents/Projects/JPL/MISR/plots/Stage2/"
res_dir <- "/Volumes/GoogleDrive/My Drive/AERONET-MISR/Paper#2-RSoE/aeronet/aeronet/results/"
setwd("/Volumes/GoogleDrive/My Drive/AERONET-MISR/Paper#2-RSoE/aeronet/aeronet/code/XGBOOST/")

```




#XGBoost
```{r,echo=F,results='hide'}
#clean working database to complete cases
dataset <- wdb.f[,c(5:19,22)]
setDF(dataset)
dataset <-Filter(function(x)!all(is.na(x)), dataset )
dataset<-dataset[which(!is.na(dataset$PM25)),]
setDT(dataset)
#exclude vars with less than 90% availability
dataset <- dataset[,(colSums(is.na(dataset)/dim(dataset)[1]))<0.1,with=FALSE]#423*59 vars
dataset<-dataset[complete.cases(dataset),]#653

#XGBoost
tune_grid <- expand.grid(nrounds = 100,
                         max_depth = 5,
                         eta = 0.05,
                         gamma = 0.01,
                         colsample_bytree = 0.75,
                         min_child_weight = 0,
                         subsample = 0.5)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10,verboseIter=TRUE)

fit.xgb <- train(PM25~., data=dataset, method = "xgbTree",
                 trControl=control,
                 tuneGrid = tune_grid,
                 tuneLength = 10, importance=TRUE)
#saveRDS(fit.xgb,paste0(res_dir,"fit.xgb.train.rds"))
```
#results
```{r}
print(fit.xgb)

```

##importance of Variables
#interpretation with SHAP-all vars
```{r}
library(xgboost)
source('~/R/Scripts/functions/shap.R')
wdb.na <- setDT(dataset)
#arrange data
names(wdb.na) <- sub("_","",names(wdb.na))
names(wdb.na) <- sub("-","",names(wdb.na))
names(wdb.na) <- sub("[","",names(wdb.na),fixed=TRUE)
names(wdb.na) <- sub("]","",names(wdb.na))
data_2 <- wdb.na[,-c("PM25")]

data_dmy = dummyVars(" ~ .", data = data_2, fullRank=T)
data_x = predict(data_dmy, newdata = data_2)

## Create the xgboost model
model_xgb = xgboost(data = data_x, 
                   nround = 10, 
                   objective="reg:linear",
                   label= wdb.na$PM25)  

## Calculate shap values
shap_result = shap.score.rank(xgb_model = model_xgb, 
                              X_train =data_x,
                              shap_approx = F)

# `shap_approx` comes from `approxcontrib` from xgboost documentation. 
# Faster but less accurate if true. Read more: help(xgboost)

## Plot var importance based on SHAP
var_importance(shap_result, top_n=10)

## Prepare data for top N variables
shap_long <- shap.prep(shap = shap_result,
                           X_train = data_x , 
                           top_n = 10)

source('~/R/Scripts/functions/plot.shap.summary.R') #Thanks to Y.LIU (MSSM)
plot.shap.summary(shap_long)#Plot shap overall metrics
tiff(paste0(res_dir,"shap_PMAOD_CA.tif"), res = 300)
plot.shap.summary(shap_long)#Plot shap overall metrics
dev.off()

## SHAP plot per variable 
tiff(paste0(res_dir,"shap1_ca_pervar.tif"))
xgb.plot.shap(data = data_x, # input data
              model = model_xgb, # xgboost model
              features = names(shap_result$mean_shap_score[1:10]), # only top 10 var
              n_col = 3, # layout option
              plot_loess = T # add red line to plot
              )
dev.off()

# Do some classical plots
#ggplotgui::ggplot_shiny(wdb.na)
```
#AERONET AOD + INV + EPA CSN

## Load the working data base
```{r}
#wdb.f<-fread("C:/Users/msorekha/Documents/Projects/JPL/MISR/Data/WDB/AOD.pm.inv_CA.csv")#739 obs
# regional subsets
wdb.f.w <-wdb.f[wdb.f$Site_Name %in% c("Fresno_2", "Fresno","DRAGON_Madera_city", "DRAGON_Tranquility","DRAGON_Bakersfield", "DRAGON_Garland", "Modesto"),]
dim(wdb.f.w)
wdb.f.c <-wdb.f[wdb.f$Site_Name %in% c("Denver_LaCasa","DRAGON_Denver_LaCasa", "DRAGON_Chatfield_Pk", "NEON_RNMP"),]
dim(wdb.f.c)

wdb.f.c2 <-wdb.f[wdb.f$Site_Name %in% c("St_Louis_University"),]
dim(wdb.f.c2)

wdb.f.e <-wdb.f[wdb.f$Site_Name %in% c("USDA_Howard","DRAGON_LAREL","DRAGON_PATUX","NASA_LaRC", "DRAGON_Essex","Big_Meadows","CCNY"),]
dim(wdb.f.e)
```

#XGBoost
```{r,echo=T,results='hide'}
#clean working database to complete cases
#wdb.f.w, wdb.f.c, wdb.f.e
setDF(wdb.f.c2)
dataset <- wdb.f.c2[,  names(wdb.f.c2)%in%vars$Var1]
#dataset[dataset=='-999']<-NA
#dataset <-Filter(function(x)!all(is.na(x)), dataset )
#dataset<-dataset[which(!is.na(dataset$PM25)),]
#exclude vars with less than 90% availability
#setDT(dataset)
#dataset <- dataset[,(colSums(is.na(dataset)/dim(dataset)[1]))<0.1,with=FALSE]#423*59 vars
#dataset<-dataset[complete.cases(dataset),]#653

#XGBoost
tune_grid <- expand.grid(nrounds = 100,
                         max_depth = 5,
                         eta = 0.05,
                         gamma = 0.01,
                         colsample_bytree = 0.75,
                         min_child_weight = 0,
                         subsample = 0.5)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10,verboseIter=TRUE)

fit.xgb <- train(PM25~., data=dataset, method = "xgbTree",
                 trControl=control,
                 tuneGrid = tune_grid,
                 tuneLength = 10, importance=TRUE)
#saveRDS(fit.xgb,paste0(res_dir,"fit.xgb.train.rds"))
pred<-predict(fit.xgb, dataset)
wdb.f.pred<-cbind(wdb.f.c2,pred)
gplot = ggplot(wdb.f.pred, aes(x=pred, y=PM25)) + geom_point() + geom_smooth(method=lm) + geom_abline() + theme_bw()
png("pm25_preds_c2.png")
print(gplot)
dev.off()
```
#results 
```{r}
print(fit.xgb)
```

##importance of Variables
#interpretation with SHAP-all vars
```{r}

wdb.na <- setDT(dataset)
#arrange data
names(wdb.na) <- sub("_","",names(wdb.na))
names(wdb.na) <- sub("-","",names(wdb.na))
names(wdb.na) <- sub("[","",names(wdb.na),fixed=TRUE)
names(wdb.na) <- sub("]","",names(wdb.na))
data_2 <- wdb.na[,-c("PM25")]

data_dmy = dummyVars(" ~ .", data = data_2, fullRank=T)
data_x = predict(data_dmy, newdata = data_2)

# Create the xgboost model
model_xgb = xgboost(data = data_x, 
                   nround = 10, 
                   objective="reg:linear",
                   label= wdb.na$PM25)  


shap_result <- shap.values(xgb_model = model_xgb, X_train = data_x)
shap_long <- shap.prep(xgb_model = model_xgb, X_train = data_x)

# **SHAP summary plot**

pdf(paste0(res_dir,"shap_PMAODINV_stl.pdf"))
shap.plot.summary(shap_long, dilute=15)
dev.off()

shap.w<-shap_result$mean_shap_score
write.csv(shap.w, paste0(res_dir,"shap_vars_w.csv"))
shap.c<-shap_result$mean_shap_score
write.csv(shap.c, paste0(res_dir,"shap_vars_c.csv"))
shap.e<-shap_result$mean_shap_score
write.csv(shap.e, paste0(res_dir,"shap_vars_e.csv"))

shap.wc <- merge(shap.w, shap.c, by=0, all=TRUE)
rownames(shap.wc) <- shap.wc$Row.names
shap.all <- merge(shap.wc, shap.e, by=0, all=TRUE)

rownames(shap.all) <- shap.all$Row.names
View(shap.all)

shap.all$sum=shap.all$x+shap.all$y.x+shap.all$y.y
shap.all<-shap.all[order(shap.all$sum),]

# linear model with top 10 ranked variables
mod = lm(PM25 ~ AODExtinctionFine870nm + AODExtinctionFine1020nm + `ExtinctionAngstrom_Exponent_440870nm-Total` +
        AsymmetryFactorCoarse440nm + StdC + AODExtinctionTotal440nm + AsymmetryFactorTotal1020nm +
          AsymmetryFactorCoarse675nm + VMRT + AODExtinctionFine440nm, data=wdb.na)

## SHAP plot per variable 
#tiff(paste0(res_dir,"shap1_ca_pervar.tif"))
#xgb.plot.shap(data = data_x, # input data
#              model = model_xgb, # xgboost model
#              features = names(shap_result$mean_shap_score[1:10]), # only top 10 var
#              n_col = 3, # layout option
#              plot_loess = T # add red line to plot
#              )
#dev.off()

```



